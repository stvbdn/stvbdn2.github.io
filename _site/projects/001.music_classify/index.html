<!DOCTYPE html>
<html>
  <head>
	<meta charset="utf-8">

	<title>Portfolio Template</title>
	<link rel="icon" type="image/png" href="/public/images/msr-student-template-favicon.png">

	<link rel="stylesheet" href="/public/stylesheets/style.css">

	<script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
</head>


  <body>

    <div id="wrapper">

      <!-- <header>
    <nav>
    	<a href="/"><h1>Austin Lawrence</h1></a>
    	<ul>
    		<li><a href="/projects">Project Portfolio</a></li>
    		<li><a href="/about/">About Me</a></li>
    		<li><a href="/blog/">Blog</a></li>
    		<li><a href="/contact/">Contact</a></li>
    	</ul>
    </nav>
</header>
 -->

      <main class="project">
	<section id="contact-content">
		<img id="project-image" src="//public/images/luxo.png">
		<h1 id="project-title">DDR - Dance Dance Robotics</h1>
		<h2 id="project-date">2014-09-29 00:00:00 -0600</h2>

		<p>By, Austin Lawrence (Ablarry91@gmail.com)
Master of Science in Robotics, Northwestern University</p>

<h2 id="intro">Intro</h2>
<p>The objective of this project is to classify music and use this information to instruct a specific dancing scheme for a robot.  The repository of this project may be found <a href="https://github.com/ablarry91/dancing_robot">here</a>.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/ZPxmVsA1itM" frameborder="0" allowfullscreen=""></iframe>

<h2 id="motivation">Motivation</h2>
<p>As consumer-level robots slowly find their place in a typical household, human personification is inevitable.  We argue with automated personal assistants, name our vacuums, and marvel at their literal interpretation of the world.  Despite this, machines of this category still lack the ability to facilitate an emotional response between the machine and a human.  To go beyond the status of an appliance, and possibly use robotics as a means of emotional therapy or entertainment, much development is still needed to create machine that reacts in a human-like manner.  Perhaps the most successful case of this may be the querky <a href="https://www.jibo.com/">Jibo</a> robot, though even it has noticeable limitations.  Though not quite analogous to emotion, having an ability to classify the genre of an mp3 music file is a step in the direction of autonomous emotion classification.</p>

<h2 id="method">Method</h2>

<p>The philosophy of this project is quite straightforward; identify information about a musical stimulus, and do something with the newfound data.  Two major techniques were implemented for the classification aspect of this project: one, a broad examination of a best-fit machine learning algorithm given musical features to estimate genre, and two, fingerprinting specific MP3 files from raw microphone samples using an array of audio peaks as data.  Both methods allow for the possibility to make an estimation about a musical stimulus, whether it is an emotional/genre characteristic, or the literal value of the song.  Provided this information, a dancing heuristic can be implemented.</p>

<h3 id="musical-classification-from-musical-features">Musical Classification from Musical Features</h3>

<p>As a quick start to this element of the project, the <a href="http://labrosa.ee.columbia.edu/millionsong/">Million Song Dataset</a> provided a foundational amount of data in the meta and analytical nature.  Thanks to the information available in this dataset, several characteristics of songs can be utilized, including:</p>

<ul>
  <li>Duration: the length of a song</li>
  <li>Artist familiarity: an estimation of how familiar the artist is to the world</li>
  <li>Artist hotness: an estimation of how popular an artist is</li>
  <li>Song hotness: an estimation of how popular a song is</li>
  <li>End of fade in: an estimation of when the song finishes fading in</li>
  <li>Start of fade out: an estimation of when the song begins to fade out</li>
  <li>Loudness: Overall loudness of a song in dB</li>
  <li>Mode: whether a song is in major or minor key</li>
  <li>Key: an estimation of the song’s key</li>
  <li>Tempo: an estimation of how fast the song is</li>
  <li>Time signature: an estimation of a song’s time signature</li>
</ul>

<p>Using Python <a href="https://www.sqlite.org/">SQLite</a> to generate the database and <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a> for learning purposes, a Bayes Net classifier was found to generate the best results, next to a J48 Decision Tree classifier.  Success is measured against the genre provided by the Million Song Dataset and the classification accuracy.</p>

<h3 id="results">Results</h3>
<p>Between decision tree, clustering, naïve Bayes, and Bayes Networks, the latter was found to generate the highest classification accuracy. At a classification accuracy of 43.7% against a ZeroR accuracy of 18.7%, much can still be done to improve the quality of this classifier.  Regardless, this implementation was successful in correctly identifying the majority class for each instance.   The most influential features were artist hotness, artist familiarity, and loudness.</p>

<h3 id="discussion">Discussion</h3>
<p>The most interesting observation to make from these results corresponds to the chronological development of these independent genres.  For example, pop has a higher misclassification for the music genre rock.  From a sequential standpoint, much of where pop is today is due to the musical influence of rock that developed prior, such as upbeat tempos, loud choruses, and popularity in mainstream media.
Additional musical features may help improve the quality of classification, provided that the features are more distinct to particular types of music.  Features not available in the dataset used in this study, such as the presence of a vocalist, deviation of tempo, and deviation of loudness may help distinguish various classes from their peers.</p>

<p>###Conclusion
Though this exercise proved useful in classifying musical genres with a heightened reliability, more work is to be done to improve accuracy to a meaningful level.  Going forward, it is of interest to transition this study into the area of feature extraction from mp3 files, as opposed to acquiring this information from another source in a precompiled format.  Doing so may influence the types of features available to study, and optimistically, improvement in the quality of musical genre classification.</p>

<p>A technical writeup of this aspect of the project may be found <a href="portfolio/assets/DDR_Final_Paper.pdf">here</a>
<img src="https://github.com/ablarry91/temp/blob/master/confusion.png?raw=true" alt="ar_sys" /></p>

<h2 id="musical-classification-from-audio-fingerprinting">Musical Classification from Audio Fingerprinting</h2>
<p>An alternative method to classification via extracted musical classes is to extract directly from the audio data itself in the form of a microphone recording.  A similar technique to what is found in Shazam’s audio fingerprinting algorithm, a Python derivation of the software was composed through the excellent work of <a href="https://github.com/worldveil/dejavu">Will Drevo</a> and made available for public use.  An easy to understand technical writeup is provided <a href="http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/">here</a> and is very similar to <a href="https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf">Avery Li-Chun Wang’s work</a>, the Chief Scientiest of Shazam.</p>

<h2 id="audio-signal-visualization">Audio Signal Visualization</h2>
<p>Another interest was to provide a visualization tool for contextually understanding the structure of an audio sample, and provide a foundation for another mode of conveying feeling to a human.  <a href="http://www.swharden.com/blog/2013-05-09-realtime-fft-audio-visualization-with-python/">Scott Harden</a> wrote an excellent script capable of publishing high resolution audio samples at 6hz, which was manipulated for this project.</p>

<h2 id="construction-of-a-ros-environment">Construction of a ROS Environment</h2>
<p>ROS was utilized to provided a unified platform for identifying a musical stimulus and acting upon it.  It relies on four significant nodes to address the entire context of this project:</p>

<ul>
  <li>
    <p>state_publisher - Maintains a class ‘SongStuff’ to store information regarding the current song identified, any meta information, and robot configuration data.  It relies on two SQLite databases: songMeta.db, and danceData.db.  Publishes JointState() messages at a specific rate, which Rviz will ultimately register.</p>
  </li>
  <li>
    <p>music_tag - Utilizes the excellent work of <a href="https://github.com/worldveil/dejavu">Will Drevo</a> to run an audio fingerprinting package given an autonomously managed fingerprint database.  Upon a publication from the ‘tag_command’ topic, a short audio sample is recorded via a microphone and a song identification is estimated and published.  Can either tag once, or in a continuous loop.</p>
  </li>
  <li>
    <p>play_music - A simple music player based from PyGame.  Convenient for testing software developements as you experiment with new things.</p>
  </li>
  <li>
    <p>GUI - A Tkinter GUI that’s aimed towards giving accessibility to the more substantial functions used in this package.  Quite configurable to publish new topics.</p>
  </li>
</ul>

<h2 id="ongoing-construction-of-a-physical-robot-model">(Ongoing) Construction of a Physical Robot Model</h2>
<p>Inspired by Disney’s inexplicable expertise in characterizing human-like characteristics in otherwise inanimate objects, a concluding goal to this project was to build a physical implementation of a robot and give it life-like gestures.  Luxo Jr., the jumping lamp in the beginning of every Pixar film, was modeled and redesigned for accomodating six independent degrees of freedom through a tension-driven servo actuator array.</p>

<p>Construction is currently ongoing, and more ROS development is anticipated to provide a simple calibration procedure, keyframing, and inverse kinematics/dynamics.</p>

<h2 id="going-forward">Going Forward</h2>
<p>It’s a continuing interest to carry this project beyond the rather linear solution of providing one explicit dance heuristic to each song.</p>

<p>Instead, a non-deterministic approach could provide interesting results that continue to leave users guessing on the robot’s next move.</p>


	</section>

</main>

      <!-- <footer>
    <ul>
    	<li>ablarry91@gmail.com</li>
    	<li>Chicago, IL</li>
    	<li></li>
    </ul>
</footer>

 -->

    </div>

  </body>
</html>
